{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9ab5548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "91d347be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, tokenizer=None, stemmer=None, lemmatizer=None, \n",
    "                 lowercase=True, remove_stopwords=True, remove_punctuation=True, remove_number=True, spell_check=True,\n",
    "                 input_filename=None, output_filename=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_number = remove_number\n",
    "        self.spell_check = spell_check\n",
    "        self.input_filename = input_filename\n",
    "        self.output_filename = output_filename\n",
    "    \n",
    "    def process(self):\n",
    "        data, labels = self.read_data(self.input_filename)\n",
    "        self._process(data, labels)\n",
    "    \n",
    "    def read_data(self, filename):\n",
    "        raw_data = pd.read_csv(filename, header=None)\n",
    "        data,labels=raw_data[1].tolist(), raw_data[0].tolist()\n",
    "        return data, labels\n",
    "    \n",
    "    def _process(self, data, labels):\n",
    "        if self.tokenizer == None:\n",
    "            raise ValueError('Error.')\n",
    "        dic = {'text': [], 'label': []}\n",
    "        spell = SpellChecker()\n",
    "        for text, label in zip(data, labels):\n",
    "            token_list = self.tokenizer.tokenize(text)\n",
    "            if self.spell_check:\n",
    "                token_list = [spell.correction(token) for token in token_list]\n",
    "                token_list = [token for token in token_list if token is not None and len(token) > 0]\n",
    "            if self.lemmatizer is not None:\n",
    "                token_list = self.lemmatize(self.lemmatizer, token_list)\n",
    "            elif self.stemmer is not None:\n",
    "                token_list = self.stem(self.stemmer, token_list)\n",
    "                \n",
    "            if self.lowercase:\n",
    "                token_list = [token.lower() for token in token_list]\n",
    "            if self.remove_stopwords:\n",
    "                token_list = [token for token in token_list if token not in nltk_stopwords]\n",
    "            if self.remove_punctuation:\n",
    "                token_list = [''.join(ch for ch in token if ch not in string.punctuation) for token in token_list]\n",
    "                token_list = [token for token in token_list if len(token) > 0]\n",
    "            if self.remove_number:\n",
    "                token_list = [''.join(ch for ch in token if ch not in [chr(48+i) for i in range(10)]) for token in token_list]\n",
    "                token_list = [token for token in token_list if len(token) > 0]\n",
    "                \n",
    "            new_text = ' '.join(token_list)\n",
    "            dic['text'].append(new_text)\n",
    "            dic['label'].append(label)\n",
    "            if len(dic['text']) % 1000 == 0:\n",
    "                print(len(dic['text']) ,f'/ {len(data)}')\n",
    "        self.generate(dic)\n",
    "    \n",
    "    def lemmatize(self, lemmatizer, token_list):\n",
    "        pos_tag_list = pos_tag(token_list)\n",
    "        for idx, (token, tag) in enumerate(pos_tag_list):\n",
    "            tag_simple = tag[0].lower()\n",
    "            if tag_simple in ['n', 'v', 'j']:\n",
    "                word_type = tag_simple.replace('j', 'a') \n",
    "            else:\n",
    "                word_type = 'n'\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, pos=word_type)\n",
    "            token_list[idx] = lemmatized_token\n",
    "        return token_list\n",
    "    \n",
    "    def stem(self, stemmer, token_list):\n",
    "        for idx, token in enumerate(token_list):\n",
    "            token_list[idx] = stemmer.stem(token)\n",
    "        return token_list\n",
    "    \n",
    "    def generate(self, dic):\n",
    "        df = pd.DataFrame(dic)\n",
    "        df.to_csv(self.output_filename)   \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c32944c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "164c865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = Preprocess(tokenizer=tweet_tokenizer, lemmatizer=wordnet_lemmatizer, spell_check=False, output_filename='../dataset/process_train.csv', \n",
    "                    input_filename='../dataset/fulltrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aee7ea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n"
     ]
    }
   ],
   "source": [
    "process.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a67529b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = Preprocess(tokenizer=tweet_tokenizer, lemmatizer=wordnet_lemmatizer, spell_check=False, output_filename='../dataset/process_test.csv', \n",
    "                    input_filename='../dataset/balancedtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "63f01903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 3000\n",
      "2000 / 3000\n",
      "3000 / 3000\n"
     ]
    }
   ],
   "source": [
    "process.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb063a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
