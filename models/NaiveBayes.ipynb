{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:/DSML/Semester 2/CS4248/Group Project/Project Code/dataset/raw_data/fulltrain.csv', names=['label', 'text'])\n",
    "X_train = train['text']\n",
    "y_train = train['label']\n",
    "tfidf = TFIDF().fit(X_train)\n",
    "X_train = tfidf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 score = 0.7810799562161455\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "# candidate_models: MultinomialNB, ComplementNB, BernoulliNB              # Train,Test\n",
    "# model = MultinomialNB()                                                 # 0.592,0.319\n",
    "# model = CalibratedClassifierCV(MultinomialNB(), cv=2, method='isotonic')# 0.791,0.550\n",
    "# model = CalibratedClassifierCV(MultinomialNB(), cv=2, method='sigmoid') # 0.675,0.408\n",
    "# model = ComplementNB()                                                  # 0.839,0.484\n",
    "# model = CalibratedClassifierCV(ComplementNB(), cv=2, method='isotonic') # 0.858,0.580\n",
    "# model = CalibratedClassifierCV(ComplementNB(), cv=2, method='sigmoid')  # 0.850.0.567\n",
    "# model = BernoulliNB()                                                   # 0.778,0.596\n",
    "# model = CalibratedClassifierCV(BernoulliNB(), cv=2, method='isotonic')  # 0.786,0.593\n",
    "model = CalibratedClassifierCV(BernoulliNB(), cv=3, method='sigmoid')  # 0.781,0.610\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# Use f1-macro as the metric\n",
    "score = f1_score(y_train, y_pred, average='macro')\n",
    "print('Train F1 score = {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score = 0.6098074991771212\n",
      "204.1195969581604 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Satire       0.61      0.75      0.68       750\n",
      "        Hoax       0.50      0.34      0.41       750\n",
      "  Propaganda       0.60      0.79      0.68       750\n",
      "    Reliable       0.78      0.59      0.68       750\n",
      "\n",
      "    accuracy                           0.62      3000\n",
      "   macro avg       0.62      0.62      0.61      3000\n",
      "weighted avg       0.62      0.62      0.61      3000\n",
      "\n",
      "[[565 183   0   2]\n",
      " [164 258 290  38]\n",
      " [ 71   4 592  83]\n",
      " [119  74 111 446]]\n"
     ]
    }
   ],
   "source": [
    "# test data for original dataset\n",
    "test = pd.read_csv('D:/DSML/Semester 2/CS4248/Group Project/Project Code/dataset/raw_data/balancedtest.csv', names=['label', 'text'])\n",
    "X_test = test['text']\n",
    "y_test = test['label']\n",
    "X_test = tfidf.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "score = f1_score(y_test, y_pred, average='macro')\n",
    "print('Test F1 score = {}'.format(score))\n",
    "print(time.time() - time0, 's')\n",
    "TEXT_LABELS = {1: 'Satire', 2: 'Hoax', 3: 'Propaganda', 4: 'Reliable'}\n",
    "class_names = list(TEXT_LABELS.values())\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score = 0.5122060008767559\n",
      "92.30877041816711 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Satire       0.76      1.00      0.86        31\n",
      "        Hoax       0.64      0.47      0.54        30\n",
      "  Propaganda       0.50      0.76      0.60        29\n",
      "    Reliable       0.08      0.03      0.05        30\n",
      "\n",
      "    accuracy                           0.57       120\n",
      "   macro avg       0.49      0.56      0.51       120\n",
      "weighted avg       0.49      0.57      0.51       120\n",
      "\n",
      "[[31  0  0  0]\n",
      " [ 1 14 10  5]\n",
      " [ 0  0 22  7]\n",
      " [ 9  8 12  1]]\n"
     ]
    }
   ],
   "source": [
    "# test data for covid dataset\n",
    "test = pd.read_csv('D:/DSML/Semester 2/CS4248/Group Project/Project Code/covid_unreliable_news_cleaned.csv')\n",
    "X_test = test['content_text']\n",
    "y_test = test['class']\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == \"Reliable News\":\n",
    "        y_test[i] = 4\n",
    "    if y_test[i] == \"Hoax\":\n",
    "        y_test[i] = 2\n",
    "    if y_test[i] == \"Propaganda\":\n",
    "        y_test[i] = 3\n",
    "    if y_test[i] == \"Satire\":\n",
    "        y_test[i] = 1\n",
    "y_test = y_test.astype(np.int64)\n",
    "X_test = tfidf.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "score = f1_score(y_test, y_pred, average='macro')\n",
    "print('Test F1 score = {}'.format(score))\n",
    "print(time.time() - time0, 's')\n",
    "TEXT_LABELS = {1: 'Satire', 2: 'Hoax', 3: 'Propaganda', 4: 'Reliable'}\n",
    "class_names = list(TEXT_LABELS.values())\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
